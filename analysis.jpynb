import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly as py
import plotly/graph_objs as go
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder

#reading the dataset
onlineshoppers_dataset=pd.read_csv('dataset.csv')

#checking the dataset
missing = onlineshoppers_dataset.isnull().sum()
print(missing)

#product related bounce rates of customers
onlineshoppers_dataset.fillna(0, inplace = True)
x=onlineshoppers_dataset.iloc[:, [5,6]].values
x.shape

#Clusterig
#Employing K-Elbow to determine the number of clustering groups, 
# PCA and t-SNE can be applied as well to determine the quality of clusters.
cg = []

# Loop over different numbers of clusters (from 1 to 10)
for i in range(1, 11):  # Range goes from 1 to 10 (11 is excluded)
    
  # Create KMeans model with specific parameters
  km = KMeans(
          n_clusters=i,            # Number of clusters to form (this should be 'n_clusters')
          init='k-means++',         # Method for initializing centroids to avoid random initialization issues
          max_iter=200,             # Maximum number of iterations for a single run
          n_init=10,                # Number of times KMeans will run with different centroid seeds and pick the best one
          random_state=0,           # Ensures reproducibility (same results every time)
          algorithm='full',         # 'full' computes exact distances; 'elkan' is faster for certain data but doesn't work with sparse matrices
          tol=0.001                 # Tolerance for stopping the algorithm if the change in inertia is less than this value
      )
    km.fit(x)
    labels = km.labels_
    cg.append(km.inertia_)


plt.rcParams['figure.figsize'] = (13,7)
plt.plot(range(1,11), cg)
plt.grid()
plt.tight_layout()
plt.title('The Elbow Method', fontsize =15) #can decide a different one 
plt.xlabel('Cluster K'),
plt.ylabel('cg')
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Create KMeans model with 2 clusters
km = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=0)

# Fit the model and predict the cluster index for each sample
y_means = km.fit_predict(x)

# Plot the clusters
plt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s=50, c='yellow', label='Uninterested Customers')
plt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s=50, c='pink', label='Target Customers')

# Plot the centroids
plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=200, c='blue', marker='X', label='Centroids')

# Add titles, labels, grid, and legend
plt.title('ProductRelated Duration vs Bounce Rate', fontsize=20)
plt.grid(True)
plt.xlabel('ProductRelated Duration')
plt.ylabel('Bounce Rates')
plt.legend()
plt.show()

# Initialize and fit LabelEncoder
le = LabelEncoder()
labels_true = le.fit_transform(data['Revenue'])  # Assuming 'Revenue' is categorical

# Get predicted clustering result labels (from your KMeans model)
labels_pred = y_means

# Calculate and print the adjusted rand index (to measure clustering performance)
score = metrics.adjusted_rand_score(labels_true, labels_pred)
print("Adjusted Rand Index:", score)

# Plot confusion matrix using scikit-plot (skplt)
# Non-normalized confusion matrix
plt_1 = skplt.metrics.plot_confusion_matrix(labels_true, labels_pred, normalize=False)
plt.title("Confusion Matrix (Unnormalized)")
plt.show()

# Normalized confusion matrix
plt_2 = skplt.metrics.plot_confusion_matrix(labels_true, labels_pred, normalize=True)
plt.title("Confusion Matrix (Normalized)")
plt.show()


